{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Classification Alzheimer - Guide Complet\n",
                "## Dataset: Images IRM pour la d√©tection des stades d'Alzheimer\n",
                "\n",
                "### Classes:\n",
                "- **NonDemented**: Pas de d√©mence\n",
                "- **VeryMildDemented**: D√©mence tr√®s l√©g√®re\n",
                "- **MildDemented**: D√©mence l√©g√®re\n",
                "- **ModerateDemented**: D√©mence mod√©r√©e\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 1: Importation des biblioth√®ques"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Biblioth√®ques de base\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import cv2\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# TensorFlow et Keras\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential, Model, load_model\n",
                "from tensorflow.keras.layers import (\n",
                "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n",
                "    BatchNormalization, GlobalAveragePooling2D, Input\n",
                ")\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
                "from tensorflow.keras.callbacks import (\n",
                "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, \n",
                "    TensorBoard, CSVLogger\n",
                ")\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
                "\n",
                "# M√©triques et √©valuation\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, \n",
                "    accuracy_score, precision_score, recall_score, f1_score\n",
                ")\n",
                "\n",
                "# Configuration\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")\n",
                "print(f\"Keras Version: {keras.__version__}\")\n",
                "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
                "\n",
                "# Seed pour la reproductibilit√©\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 2: Configuration des chemins et param√®tres"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chemins des donn√©es\n",
                "BASE_DIR = Path('AlzheimerDataset')\n",
                "TRAIN_DIR = BASE_DIR / 'train'\n",
                "TEST_DIR = BASE_DIR / 'test'\n",
                "\n",
                "# Chemins de sauvegarde\n",
                "MODEL_DIR = Path('AlzheimerModel')\n",
                "MODELS_DIR = MODEL_DIR / 'models'\n",
                "GRAPHS_DIR = MODEL_DIR / 'graphs'\n",
                "LOGS_DIR = MODEL_DIR / 'logs'\n",
                "\n",
                "# Cr√©er les dossiers s'ils n'existent pas\n",
                "for directory in [MODELS_DIR, GRAPHS_DIR, LOGS_DIR]:\n",
                "    directory.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Param√®tres du mod√®le\n",
                "IMG_SIZE = (176, 176)  # Taille des images\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 50\n",
                "LEARNING_RATE = 0.0001\n",
                "\n",
                "# Classes\n",
                "CLASSES = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n",
                "NUM_CLASSES = len(CLASSES)\n",
                "\n",
                "print(f\"Train Directory: {TRAIN_DIR}\")\n",
                "print(f\"Test Directory: {TEST_DIR}\")\n",
                "print(f\"Number of Classes: {NUM_CLASSES}\")\n",
                "print(f\"Classes: {CLASSES}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 3: Exploration des donn√©es"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fonction pour compter les images par classe\n",
                "def count_images(directory):\n",
                "    \"\"\"Compte le nombre d'images dans chaque classe\"\"\"\n",
                "    data = {'Class': [], 'Count': []}\n",
                "    \n",
                "    for class_name in CLASSES:\n",
                "        class_path = directory / class_name\n",
                "        if class_path.exists():\n",
                "            count = len(list(class_path.glob('*.jpg'))) + len(list(class_path.glob('*.png')))\n",
                "            data['Class'].append(class_name)\n",
                "            data['Count'].append(count)\n",
                "            print(f\"{class_name}: {count} images\")\n",
                "    \n",
                "    return pd.DataFrame(data)\n",
                "\n",
                "# Compter les images\n",
                "print(\"\\n=== TRAIN SET ===\")\n",
                "train_df = count_images(TRAIN_DIR)\n",
                "\n",
                "print(\"\\n=== TEST SET ===\")\n",
                "test_df = count_images(TEST_DIR)\n",
                "\n",
                "print(f\"\\nTotal Train Images: {train_df['Count'].sum()}\")\n",
                "print(f\"Total Test Images: {test_df['Count'].sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 4: Visualisation de la distribution des donn√©es"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualiser la distribution des classes\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Train set\n",
                "sns.barplot(data=train_df, x='Class', y='Count', ax=axes[0], palette='viridis')\n",
                "axes[0].set_title('Distribution des classes - Train Set', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Classe', fontsize=12)\n",
                "axes[0].set_ylabel('Nombre d\\'images', fontsize=12)\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# Test set\n",
                "sns.barplot(data=test_df, x='Class', y='Count', ax=axes[1], palette='magma')\n",
                "axes[1].set_title('Distribution des classes - Test Set', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Classe', fontsize=12)\n",
                "axes[1].set_ylabel('Nombre d\\'images', fontsize=12)\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(GRAPHS_DIR / 'class_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 5: Visualisation d'exemples d'images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Afficher des exemples d'images pour chaque classe\n",
                "fig, axes = plt.subplots(4, 4, figsize=(15, 15))\n",
                "\n",
                "for i, class_name in enumerate(CLASSES):\n",
                "    class_path = TRAIN_DIR / class_name\n",
                "    images = list(class_path.glob('*.jpg')) + list(class_path.glob('*.png'))\n",
                "    \n",
                "    # Afficher 4 images par classe\n",
                "    for j in range(4):\n",
                "        if j < len(images):\n",
                "            img = cv2.imread(str(images[j]))\n",
                "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "            axes[i, j].imshow(img)\n",
                "            axes[i, j].set_title(f'{class_name}', fontsize=10)\n",
                "        axes[i, j].axis('off')\n",
                "\n",
                "plt.suptitle('Exemples d\\'images IRM par classe', fontsize=16, fontweight='bold', y=0.995)\n",
                "plt.tight_layout()\n",
                "plt.savefig(GRAPHS_DIR / 'sample_images.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 6: Pr√©paration des donn√©es avec augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# G√©n√©rateur d'augmentation pour les donn√©es d'entra√Ænement\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1./255,                # Normalisation\n",
                "    rotation_range=20,             # Rotation al√©atoire\n",
                "    width_shift_range=0.2,         # D√©calage horizontal\n",
                "    height_shift_range=0.2,        # D√©calage vertical\n",
                "    horizontal_flip=True,          # Retournement horizontal\n",
                "    zoom_range=0.2,                # Zoom al√©atoire\n",
                "    shear_range=0.2,               # Cisaillement\n",
                "    fill_mode='nearest',           # Remplissage des pixels\n",
                "    validation_split=0.2           # 20% pour validation\n",
                ")\n",
                "\n",
                "# G√©n√©rateur pour les donn√©es de test (seulement normalisation)\n",
                "test_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "# Cr√©er les g√©n√©rateurs\n",
                "print(\"Cr√©ation des g√©n√©rateurs de donn√©es...\\n\")\n",
                "\n",
                "# Train generator\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    str(TRAIN_DIR),\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical',\n",
                "    subset='training',\n",
                "    shuffle=True,\n",
                "    seed=42\n",
                ")\n",
                "\n",
                "# Validation generator\n",
                "validation_generator = train_datagen.flow_from_directory(\n",
                "    str(TRAIN_DIR),\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical',\n",
                "    subset='validation',\n",
                "    shuffle=True,\n",
                "    seed=42\n",
                ")\n",
                "\n",
                "# Test generator\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    str(TEST_DIR),\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical',\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "print(f\"\\nClasses: {train_generator.class_indices}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 7: Construction du mod√®le CNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_cnn_model(input_shape=(*IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
                "    \"\"\"\n",
                "    Cr√©e un mod√®le CNN personnalis√© pour la classification Alzheimer\n",
                "    \"\"\"\n",
                "    model = Sequential([\n",
                "        # Premier bloc convolutionnel\n",
                "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
                "        BatchNormalization(),\n",
                "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D(pool_size=(2, 2)),\n",
                "        Dropout(0.25),\n",
                "        \n",
                "        # Deuxi√®me bloc convolutionnel\n",
                "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D(pool_size=(2, 2)),\n",
                "        Dropout(0.25),\n",
                "        \n",
                "        # Troisi√®me bloc convolutionnel\n",
                "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D(pool_size=(2, 2)),\n",
                "        Dropout(0.25),\n",
                "        \n",
                "        # Quatri√®me bloc convolutionnel\n",
                "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
                "        BatchNormalization(),\n",
                "        MaxPooling2D(pool_size=(2, 2)),\n",
                "        Dropout(0.25),\n",
                "        \n",
                "        # Couches denses\n",
                "        Flatten(),\n",
                "        Dense(512, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.5),\n",
                "        \n",
                "        Dense(256, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.5),\n",
                "        \n",
                "        # Couche de sortie\n",
                "        Dense(num_classes, activation='softmax')\n",
                "    ])\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Cr√©er le mod√®le\n",
                "model = create_cnn_model()\n",
                "\n",
                "# Afficher l'architecture\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 8: Compilation du mod√®le"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compiler le mod√®le\n",
                "model.compile(\n",
                "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
                "    loss='categorical_crossentropy',\n",
                "    metrics=['accuracy', \n",
                "             tf.keras.metrics.Precision(name='precision'),\n",
                "             tf.keras.metrics.Recall(name='recall')]\n",
                ")\n",
                "\n",
                "print(\"Mod√®le compil√© avec succ√®s!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 9: Configuration des callbacks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# D√©finir les callbacks\n",
                "callbacks = [\n",
                "    # Sauvegarde du meilleur mod√®le\n",
                "    ModelCheckpoint(\n",
                "        filepath=str(MODELS_DIR / 'best_model.h5'),\n",
                "        monitor='val_accuracy',\n",
                "        mode='max',\n",
                "        save_best_only=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    \n",
                "    # Arr√™t anticip√© si pas d'am√©lioration\n",
                "    EarlyStopping(\n",
                "        monitor='val_loss',\n",
                "        patience=10,\n",
                "        restore_best_weights=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    \n",
                "    # R√©duction du learning rate si plateau\n",
                "    ReduceLROnPlateau(\n",
                "        monitor='val_loss',\n",
                "        factor=0.5,\n",
                "        patience=5,\n",
                "        min_lr=1e-7,\n",
                "        verbose=1\n",
                "    ),\n",
                "    \n",
                "    # TensorBoard pour visualisation\n",
                "    TensorBoard(\n",
                "        log_dir=str(LOGS_DIR),\n",
                "        histogram_freq=1\n",
                "    ),\n",
                "    \n",
                "    # Sauvegarde de l'historique en CSV\n",
                "    CSVLogger(\n",
                "        filename=str(LOGS_DIR / 'training_log.csv'),\n",
                "        separator=',',\n",
                "        append=False\n",
                "    )\n",
                "]\n",
                "\n",
                "print(\"Callbacks configur√©s:\")\n",
                "for callback in callbacks:\n",
                "    print(f\"  - {callback.__class__.__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 10: Entra√Ænement du mod√®le"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculer les steps\n",
                "STEPS_PER_EPOCH = train_generator.samples // BATCH_SIZE\n",
                "VALIDATION_STEPS = validation_generator.samples // BATCH_SIZE\n",
                "\n",
                "print(f\"Steps per epoch: {STEPS_PER_EPOCH}\")\n",
                "print(f\"Validation steps: {VALIDATION_STEPS}\")\n",
                "print(f\"\\nD√©but de l'entra√Ænement...\\n\")\n",
                "\n",
                "# Entra√Æner le mod√®le\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    steps_per_epoch=STEPS_PER_EPOCH,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=validation_generator,\n",
                "    validation_steps=VALIDATION_STEPS,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"\\nEntra√Ænement termin√©!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 11: Visualisation de l'historique d'entra√Ænement"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fonction pour tracer l'historique\n",
                "def plot_training_history(history):\n",
                "    \"\"\"\n",
                "    Trace les courbes d'apprentissage\n",
                "    \"\"\"\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "    \n",
                "    # Accuracy\n",
                "    axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
                "    axes[0, 0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
                "    axes[0, 0].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
                "    axes[0, 0].set_xlabel('Epoch')\n",
                "    axes[0, 0].set_ylabel('Accuracy')\n",
                "    axes[0, 0].legend()\n",
                "    axes[0, 0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Loss\n",
                "    axes[0, 1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
                "    axes[0, 1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
                "    axes[0, 1].set_title('Loss', fontsize=14, fontweight='bold')\n",
                "    axes[0, 1].set_xlabel('Epoch')\n",
                "    axes[0, 1].set_ylabel('Loss')\n",
                "    axes[0, 1].legend()\n",
                "    axes[0, 1].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Precision\n",
                "    axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
                "    axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n",
                "    axes[1, 0].set_title('Precision', fontsize=14, fontweight='bold')\n",
                "    axes[1, 0].set_xlabel('Epoch')\n",
                "    axes[1, 0].set_ylabel('Precision')\n",
                "    axes[1, 0].legend()\n",
                "    axes[1, 0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Recall\n",
                "    axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2)\n",
                "    axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2)\n",
                "    axes[1, 1].set_title('Recall', fontsize=14, fontweight='bold')\n",
                "    axes[1, 1].set_xlabel('Epoch')\n",
                "    axes[1, 1].set_ylabel('Recall')\n",
                "    axes[1, 1].legend()\n",
                "    axes[1, 1].grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(GRAPHS_DIR / 'training_history.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "\n",
                "# Tracer l'historique\n",
                "plot_training_history(history)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 12: √âvaluation sur le test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# √âvaluer sur le test set\n",
                "print(\"√âvaluation sur le test set...\\n\")\n",
                "\n",
                "test_loss, test_acc, test_precision, test_recall = model.evaluate(\n",
                "    test_generator,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Calculer le F1-score\n",
                "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"R√âSULTATS SUR LE TEST SET\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Test Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
                "print(f\"Test Loss:      {test_loss:.4f}\")\n",
                "print(f\"Test Precision: {test_precision:.4f}\")\n",
                "print(f\"Test Recall:    {test_recall:.4f}\")\n",
                "print(f\"Test F1-Score:  {test_f1:.4f}\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 13: Matrice de confusion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pr√©dictions sur le test set\n",
                "print(\"G√©n√©ration des pr√©dictions...\")\n",
                "y_pred_probs = model.predict(test_generator, verbose=1)\n",
                "y_pred = np.argmax(y_pred_probs, axis=1)\n",
                "y_true = test_generator.classes\n",
                "\n",
                "# Matrice de confusion\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "# Visualiser la matrice de confusion\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(\n",
                "    cm, \n",
                "    annot=True, \n",
                "    fmt='d', \n",
                "    cmap='Blues',\n",
                "    xticklabels=CLASSES,\n",
                "    yticklabels=CLASSES,\n",
                "    cbar_kws={'label': 'Nombre de pr√©dictions'}\n",
                ")\n",
                "plt.title('Matrice de Confusion', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.ylabel('Vraie classe', fontsize=12)\n",
                "plt.xlabel('Classe pr√©dite', fontsize=12)\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.savefig(GRAPHS_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Matrice de confusion normalis√©e\n",
                "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(\n",
                "    cm_normalized, \n",
                "    annot=True, \n",
                "    fmt='.2%', \n",
                "    cmap='YlOrRd',\n",
                "    xticklabels=CLASSES,\n",
                "    yticklabels=CLASSES,\n",
                "    cbar_kws={'label': 'Pourcentage'}\n",
                ")\n",
                "plt.title('Matrice de Confusion Normalis√©e', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.ylabel('Vraie classe', fontsize=12)\n",
                "plt.xlabel('Classe pr√©dite', fontsize=12)\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.savefig(GRAPHS_DIR / 'confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 14: Rapport de classification d√©taill√©"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Rapport de classification\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"RAPPORT DE CLASSIFICATION D√âTAILL√â\")\n",
                "print(\"=\"*70)\n",
                "print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 15: Sauvegarde du mod√®le final"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sauvegarder le mod√®le complet\n",
                "model_path = MODELS_DIR / 'alzheimer_model_final.h5'\n",
                "model.save(str(model_path))\n",
                "print(f\"‚úì Mod√®le complet sauvegard√©: {model_path}\")\n",
                "\n",
                "# Sauvegarder les poids uniquement\n",
                "weights_path = MODELS_DIR / 'alzheimer_weights.h5'\n",
                "model.save_weights(str(weights_path))\n",
                "print(f\"‚úì Poids sauvegard√©s: {weights_path}\")\n",
                "\n",
                "# Sauvegarder l'architecture en JSON\n",
                "architecture_path = MODELS_DIR / 'model_architecture.json'\n",
                "with open(architecture_path, 'w') as f:\n",
                "    f.write(model.to_json())\n",
                "print(f\"‚úì Architecture sauvegard√©e: {architecture_path}\")\n",
                "\n",
                "# Sauvegarder l'historique d'entra√Ænement\n",
                "history_df = pd.DataFrame(history.history)\n",
                "history_csv_path = LOGS_DIR / 'training_history.csv'\n",
                "history_df.to_csv(history_csv_path, index=False)\n",
                "print(f\"‚úì Historique sauvegard√©: {history_csv_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 16: Fonction de pr√©diction pour nouvelles images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_alzheimer_stage(image_path, model, img_size=IMG_SIZE):\n",
                "    \"\"\"\n",
                "    Pr√©dit le stade d'Alzheimer pour une nouvelle image IRM\n",
                "    \n",
                "    Args:\n",
                "        image_path: Chemin vers l'image\n",
                "        model: Mod√®le entra√Æn√©\n",
                "        img_size: Taille de l'image\n",
                "    \n",
                "    Returns:\n",
                "        dict: Pr√©dictions avec probabilit√©s pour chaque classe\n",
                "    \"\"\"\n",
                "    # Charger et pr√©traiter l'image\n",
                "    img = load_img(image_path, target_size=img_size)\n",
                "    img_array = img_to_array(img)\n",
                "    img_array = np.expand_dims(img_array, axis=0)\n",
                "    img_array = img_array / 255.0\n",
                "    \n",
                "    # Pr√©diction\n",
                "    predictions = model.predict(img_array, verbose=0)\n",
                "    predicted_class_idx = np.argmax(predictions[0])\n",
                "    predicted_class = CLASSES[predicted_class_idx]\n",
                "    confidence = predictions[0][predicted_class_idx] * 100\n",
                "    \n",
                "    # Cr√©er un dictionnaire avec toutes les probabilit√©s\n",
                "    probabilities = {CLASSES[i]: float(predictions[0][i] * 100) for i in range(NUM_CLASSES)}\n",
                "    \n",
                "    return {\n",
                "        'predicted_class': predicted_class,\n",
                "        'confidence': confidence,\n",
                "        'probabilities': probabilities,\n",
                "        'image': img\n",
                "    }\n",
                "\n",
                "def visualize_prediction(image_path, model):\n",
                "    \"\"\"\n",
                "    Visualise la pr√©diction avec l'image et les probabilit√©s\n",
                "    \"\"\"\n",
                "    result = predict_alzheimer_stage(image_path, model)\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Afficher l'image\n",
                "    axes[0].imshow(result['image'])\n",
                "    axes[0].set_title(f\"Pr√©diction: {result['predicted_class']}\\nConfiance: {result['confidence']:.2f}%\",\n",
                "                     fontsize=12, fontweight='bold')\n",
                "    axes[0].axis('off')\n",
                "    \n",
                "    # Afficher les probabilit√©s\n",
                "    classes = list(result['probabilities'].keys())\n",
                "    probs = list(result['probabilities'].values())\n",
                "    colors = ['green' if c == result['predicted_class'] else 'gray' for c in classes]\n",
                "    \n",
                "    axes[1].barh(classes, probs, color=colors, alpha=0.7)\n",
                "    axes[1].set_xlabel('Probabilit√© (%)', fontsize=11)\n",
                "    axes[1].set_title('Probabilit√©s par classe', fontsize=12, fontweight='bold')\n",
                "    axes[1].set_xlim(0, 100)\n",
                "    \n",
                "    # Ajouter les valeurs sur les barres\n",
                "    for i, (c, p) in enumerate(zip(classes, probs)):\n",
                "        axes[1].text(p + 1, i, f'{p:.2f}%', va='center', fontsize=9)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    return result\n",
                "\n",
                "print(\"Fonctions de pr√©diction d√©finies!\")\n",
                "print(\"\\nUtilisation:\")\n",
                "print(\"  result = predict_alzheimer_stage('path/to/image.jpg', model)\")\n",
                "print(\"  visualize_prediction('path/to/image.jpg', model)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 17: Test de pr√©diction sur des exemples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tester la pr√©diction sur quelques images du test set\n",
                "print(\"Test de pr√©diction sur des exemples du test set:\\n\")\n",
                "\n",
                "# S√©lectionner une image al√©atoire de chaque classe\n",
                "for class_name in CLASSES:\n",
                "    class_path = TEST_DIR / class_name\n",
                "    images = list(class_path.glob('*.jpg')) + list(class_path.glob('*.png'))\n",
                "    \n",
                "    if images:\n",
                "        # Prendre une image al√©atoire\n",
                "        random_image = np.random.choice(images)\n",
                "        print(f\"\\n{'='*60}\")\n",
                "        print(f\"Vraie classe: {class_name}\")\n",
                "        print(f\"Image: {random_image.name}\")\n",
                "        print(f\"{'='*60}\")\n",
                "        \n",
                "        result = visualize_prediction(str(random_image), model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 18: R√©sum√© final et statistiques"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cr√©er un r√©sum√© final\n",
                "summary = f\"\"\"\n",
                "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
                "‚ïë          R√âSUM√â DU PROJET - CLASSIFICATION ALZHEIMER        ‚ïë\n",
                "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
                "‚ïë                                                              ‚ïë\n",
                "‚ïë  DATASET                                                     ‚ïë\n",
                "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                    ‚ïë\n",
                "‚ïë  ‚Ä¢ Nombre de classes: {NUM_CLASSES}                                        ‚ïë\n",
                "‚ïë  ‚Ä¢ Classes: {', '.join(CLASSES[:2])}                         ‚ïë\n",
                "‚ïë            {', '.join(CLASSES[2:])}                          ‚ïë\n",
                "‚ïë  ‚Ä¢ Images d'entra√Ænement: {train_df['Count'].sum()}                              ‚ïë\n",
                "‚ïë  ‚Ä¢ Images de test: {test_df['Count'].sum()}                                    ‚ïë\n",
                "‚ïë                                                              ‚ïë\n",
                "‚ïë  MOD√àLE                                                      ‚ïë\n",
                "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                      ‚ïë\n",
                "‚ïë  ‚Ä¢ Architecture: CNN personnalis√©                            ‚ïë\n",
                "‚ïë  ‚Ä¢ Param√®tres totaux: {model.count_params():,}                       ‚ïë\n",
                "‚ïë  ‚Ä¢ Taille d'image: {IMG_SIZE[0]}x{IMG_SIZE[1]}                                    ‚ïë\n",
                "‚ïë  ‚Ä¢ Batch size: {BATCH_SIZE}                                           ‚ïë\n",
                "‚ïë                                                              ‚ïë\n",
                "‚ïë  ENTRA√éNEMENT                                                ‚ïë\n",
                "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                              ‚ïë\n",
                "‚ïë  ‚Ä¢ Epochs: {len(history.history['loss'])}                                             ‚ïë\n",
                "‚ïë  ‚Ä¢ Learning rate: {LEARNING_RATE}                                   ‚ïë\n",
                "‚ïë  ‚Ä¢ Optimizer: Adam                                           ‚ïë\n",
                "‚ïë  ‚Ä¢ Data augmentation: Oui                                    ‚ïë\n",
                "‚ïë                                                              ‚ïë\n",
                "‚ïë  PERFORMANCES                                                ‚ïë\n",
                "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                              ‚ïë\n",
                "‚ïë  ‚Ä¢ Test Accuracy:  {test_acc*100:.2f}%                                  ‚ïë\n",
                "‚ïë  ‚Ä¢ Test Precision: {test_precision*100:.2f}%                                  ‚ïë\n",
                "‚ïë  ‚Ä¢ Test Recall:    {test_recall*100:.2f}%                                  ‚ïë\n",
                "‚ïë  ‚Ä¢ Test F1-Score:  {test_f1*100:.2f}%                                  ‚ïë\n",
                "‚ïë                                                              ‚ïë\n",
                "‚ïë  FICHIERS SAUVEGARD√âS                                        ‚ïë\n",
                "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                           ‚ïë\n",
                "‚ïë  ‚Ä¢ Mod√®le complet: alzheimer_model_final.h5                 ‚ïë\n",
                "‚ïë  ‚Ä¢ Meilleur mod√®le: best_model.h5                           ‚ïë\n",
                "‚ïë  ‚Ä¢ Poids: alzheimer_weights.h5                              ‚ïë\n",
                "‚ïë  ‚Ä¢ Architecture: model_architecture.json                    ‚ïë\n",
                "‚ïë  ‚Ä¢ Logs: training_log.csv, training_history.csv             ‚ïë\n",
                "‚ïë  ‚Ä¢ Graphiques: confusion_matrix.png, training_history.png   ‚ïë\n",
                "‚ïë                                                              ‚ïë\n",
                "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
                "\"\"\"\n",
                "\n",
                "print(summary)\n",
                "\n",
                "# Sauvegarder le r√©sum√©\n",
                "with open(MODELS_DIR / 'training_summary.txt', 'w', encoding='utf-8') as f:\n",
                "    f.write(summary)\n",
                "\n",
                "print(\"\\n‚úì R√©sum√© sauvegard√© dans: AlzheimerModel/models/training_summary.txt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## √âtape 19: Charger un mod√®le sauvegard√© (pour utilisation future)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Code pour charger un mod√®le sauvegard√©\n",
                "def load_saved_model(model_path):\n",
                "    \"\"\"\n",
                "    Charge un mod√®le sauvegard√©\n",
                "    \"\"\"\n",
                "    model = load_model(model_path)\n",
                "    print(f\"‚úì Mod√®le charg√© depuis: {model_path}\")\n",
                "    return model\n",
                "\n",
                "# Exemple d'utilisation:\n",
                "# loaded_model = load_saved_model('AlzheimerModel/models/alzheimer_model_final.h5')\n",
                "# result = predict_alzheimer_stage('path/to/new/image.jpg', loaded_model)\n",
                "\n",
                "print(\"Fonction de chargement d√©finie!\")\n",
                "print(\"\\nPour charger un mod√®le:\")\n",
                "print(\"  loaded_model = load_saved_model('AlzheimerModel/models/alzheimer_model_final.h5')\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üéâ Notebook termin√©!\n",
                "\n",
                "### Prochaines √©tapes:\n",
                "1. **Am√©liorer le mod√®le**: Essayer Transfer Learning (VGG16, ResNet, etc.)\n",
                "2. **Optimiser**: Tuning des hyperparam√®tres\n",
                "3. **D√©ployer**: Cr√©er une application web pour les pr√©dictions\n",
                "4. **Analyser**: √âtudier les erreurs de classification\n",
                "\n",
                "### Utilisation du mod√®le:\n",
                "```python\n",
                "# Charger le mod√®le\n",
                "model = load_saved_model('AlzheimerModel/models/alzheimer_model_final.h5')\n",
                "\n",
                "# Faire une pr√©diction\n",
                "result = predict_alzheimer_stage('path/to/mri_image.jpg', model)\n",
                "print(f\"Pr√©diction: {result['predicted_class']}\")\n",
                "print(f\"Confiance: {result['confidence']:.2f}%\")\n",
                "\n",
                "# Ou visualiser\n",
                "visualize_prediction('path/to/mri_image.jpg', model)\n",
                "```\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}