{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Classification Alzheimer - Kaggle Optimized\n",
    "## Dataset: Images IRM pour la d√©tection des stades d'Alzheimer\n",
    "\n",
    "### Classes:\n",
    "- **NonDemented**: Pas de d√©mence\n",
    "- **VeryMildDemented**: D√©mence tr√®s l√©g√®re  \n",
    "- **MildDemented**: D√©mence l√©g√®re\n",
    "- **ModerateDemented**: D√©mence mod√©r√©e\n",
    "\n",
    "---\n",
    "**‚ö° Optimis√© pour Kaggle avec GPU P100/T4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration Kaggle & V√©rification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier l'environnement Kaggle\n",
    "import os\n",
    "print(\"üìÅ R√©pertoire actuel:\", os.getcwd())\n",
    "print(\"\\nüìÇ Datasets disponibles dans /kaggle/input:\")\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(dirname)\n",
    "    if len(filenames) > 0:\n",
    "        for filename in filenames[:3]:\n",
    "            print(f\"  - {filename}\")\n",
    "\n",
    "# V√©rifier GPU\n",
    "import tensorflow as tf\n",
    "print(\"\\nüéÆ GPUs disponibles:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"üî¢ TensorFlow version:\", tf.__version__)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"‚úÖ GPU ACTIV√â - Entra√Ænement rapide!\")\n",
    "    # Optimisation m√©moire GPU\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CPU uniquement - Activez le GPU dans Settings > Accelerator!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Importation des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Seed pour reproductibilit√©\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Configuration des chemins KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: Modifiez ces chemins selon votre dataset Kaggle\n",
    "# Option 1: Dataset upload√© personnellement\n",
    "TRAIN_DIR = '/kaggle/input/alzheimer-mri-dataset/train'\n",
    "TEST_DIR = '/kaggle/input/alzheimer-mri-dataset/test'\n",
    "\n",
    "# Option 2: Dataset public Kaggle (d√©commentez si vous utilisez celui-ci)\n",
    "# TRAIN_DIR = '/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train'\n",
    "# TEST_DIR = '/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test'\n",
    "\n",
    "# Chemins de sauvegarde (dans /kaggle/working pour t√©l√©chargement)\n",
    "OUTPUT_DIR = Path('/kaggle/working')\n",
    "\n",
    "# Param√®tres\n",
    "IMG_SIZE = (176, 176)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "CLASSES = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"Train Directory: {TRAIN_DIR}\")\n",
    "print(f\"Test Directory: {TEST_DIR}\")\n",
    "print(f\"Number of Classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exploration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(directory):\n",
    "    data = {'Class': [], 'Count': []}\n",
    "    for class_name in CLASSES:\n",
    "        class_path = Path(directory) / class_name\n",
    "        if class_path.exists():\n",
    "            count = len(list(class_path.glob('*.jpg'))) + len(list(class_path.glob('*.png')))\n",
    "            data['Class'].append(class_name)\n",
    "            data['Count'].append(count)\n",
    "            print(f\"{class_name}: {count} images\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print(\"=== TRAIN SET ===\")\n",
    "train_df = count_images(TRAIN_DIR)\n",
    "\n",
    "print(\"\\n=== TEST SET ===\")\n",
    "test_df = count_images(TEST_DIR)\n",
    "\n",
    "print(f\"\\nTotal Train: {train_df['Count'].sum()}\")\n",
    "print(f\"Total Test: {test_df['Count'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualisation distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.barplot(data=train_df, x='Class', y='Count', ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Distribution Train Set', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.barplot(data=test_df, x='Class', y='Count', ax=axes[1], palette='magma')\n",
    "axes[1].set_title('Distribution Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'class_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Exemples d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(15, 15))\n",
    "\n",
    "for i, class_name in enumerate(CLASSES):\n",
    "    class_path = Path(TRAIN_DIR) / class_name\n",
    "    images = list(class_path.glob('*.jpg')) + list(class_path.glob('*.png'))\n",
    "    \n",
    "    for j in range(4):\n",
    "        if j < len(images):\n",
    "            img = cv2.imread(str(images[j]))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            axes[i, j].imshow(img)\n",
    "            axes[i, j].set_title(f'{class_name}', fontsize=10)\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.suptitle('Exemples IRM par classe', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'sample_images.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Pr√©paration des donn√©es avec augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"Cr√©ation des g√©n√©rateurs...\\n\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nClasses: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Construction du mod√®le CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        # Bloc 1\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(*IMG_SIZE, 3)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Bloc 2\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Bloc 3\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Bloc 4\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Dense\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', \n",
    "             tf.keras.metrics.Precision(name='precision'),\n",
    "             tf.keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le compil√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìû Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(OUTPUT_DIR / 'best_model.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    CSVLogger(\n",
    "        filename=str(OUTPUT_DIR / 'training_log.csv'),\n",
    "        separator=',',\n",
    "        append=False\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Callbacks configur√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = train_generator.samples // BATCH_SIZE\n",
    "VALIDATION_STEPS = validation_generator.samples // BATCH_SIZE\n",
    "\n",
    "print(f\"Steps per epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"Validation steps: {VALIDATION_STEPS}\")\n",
    "print(f\"\\nüöÄ D√©but entra√Ænement...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualisation historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[0, 0].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0, 1].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history.history['precision'], label='Train', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val', linewidth=2)\n",
    "axes[1, 0].set_title('Precision', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(history.history['recall'], label='Train', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val', linewidth=2)\n",
    "axes[1, 1].set_title('Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ √âvaluation Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"√âvaluation sur le test set...\\n\")\n",
    "\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(test_generator, verbose=1)\n",
    "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"R√âSULTATS TEST SET\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Loss:      {test_loss:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî≤ Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"G√©n√©ration des pr√©dictions...\")\n",
    "y_pred_probs = model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "plt.title('Matrice de Confusion', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe pr√©dite')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RAPPORT DE CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Sauvegarde du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder dans /kaggle/working pour t√©l√©chargement\n",
    "model.save(OUTPUT_DIR / 'alzheimer_model.h5')\n",
    "print(\"‚úÖ Mod√®le sauvegard√©: alzheimer_model.h5\")\n",
    "\n",
    "model.save_weights(OUTPUT_DIR / 'alzheimer_weights.h5')\n",
    "print(\"‚úÖ Poids sauvegard√©s: alzheimer_weights.h5\")\n",
    "\n",
    "# Sauvegarder historique\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(OUTPUT_DIR / 'history.csv', index=False)\n",
    "print(\"‚úÖ Historique sauvegard√©: history.csv\")\n",
    "\n",
    "import os\n",
    "size_mb = os.path.getsize(OUTPUT_DIR / 'alzheimer_model.h5') / (1024*1024)\n",
    "print(f\"\\nüìä Taille mod√®le: {size_mb:.2f} MB\")\n",
    "print(\"\\nüì• T√©l√©charger depuis l'onglet Output (droite) ‚Üí\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Fonction de pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_alzheimer(image_path, model):\n",
    "    img = load_img(image_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    predictions = model.predict(img_array, verbose=0)\n",
    "    predicted_idx = np.argmax(predictions[0])\n",
    "    predicted_class = CLASSES[predicted_idx]\n",
    "    confidence = predictions[0][predicted_idx] * 100\n",
    "    \n",
    "    probabilities = {CLASSES[i]: float(predictions[0][i] * 100) for i in range(NUM_CLASSES)}\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': predicted_class,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': probabilities\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Fonction de pr√©diction d√©finie\")\n",
    "print(\"\\nUtilisation:\")\n",
    "print(\"  result = predict_alzheimer('path/to/image.jpg', model)\")\n",
    "print(\"  print(result['predicted_class'], result['confidence'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù R√©sum√© final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë     CLASSIFICATION ALZHEIMER - R√âSUM√â FINAL      ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                   ‚ïë\n",
    "‚ïë  üìä Dataset                                       ‚ïë\n",
    "‚ïë     ‚Ä¢ Classes: {NUM_CLASSES}                                     ‚ïë\n",
    "‚ïë     ‚Ä¢ Train: {train_df['Count'].sum()}                                    ‚ïë\n",
    "‚ïë     ‚Ä¢ Test: {test_df['Count'].sum()}                                     ‚ïë\n",
    "‚ïë                                                   ‚ïë\n",
    "‚ïë  üèóÔ∏è Mod√®le                                        ‚ïë\n",
    "‚ïë     ‚Ä¢ Param√®tres: {model.count_params():,}                      ‚ïë\n",
    "‚ïë     ‚Ä¢ Image size: {IMG_SIZE[0]}x{IMG_SIZE[1]}                            ‚ïë\n",
    "‚ïë     ‚Ä¢ Epochs: {len(history.history['loss'])}                                     ‚ïë\n",
    "‚ïë                                                   ‚ïë\n",
    "‚ïë  üìà Performances                                  ‚ïë\n",
    "‚ïë     ‚Ä¢ Accuracy:  {test_acc*100:.2f}%                             ‚ïë\n",
    "‚ïë     ‚Ä¢ Precision: {test_precision*100:.2f}%                             ‚ïë\n",
    "‚ïë     ‚Ä¢ Recall:    {test_recall*100:.2f}%                             ‚ïë\n",
    "‚ïë     ‚Ä¢ F1-Score:  {test_f1*100:.2f}%                             ‚ïë\n",
    "‚ïë                                                   ‚ïë\n",
    "‚ïë  üíæ Fichiers sauvegard√©s                          ‚ïë\n",
    "‚ïë     ‚Ä¢ alzheimer_model.h5                         ‚ïë\n",
    "‚ïë     ‚Ä¢ alzheimer_weights.h5                       ‚ïë\n",
    "‚ïë     ‚Ä¢ best_model.h5                              ‚ïë\n",
    "‚ïë     ‚Ä¢ training_log.csv                           ‚ïë\n",
    "‚ïë     ‚Ä¢ history.csv                                ‚ïë\n",
    "‚ïë                                                   ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(OUTPUT_DIR / 'summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"‚úÖ R√©sum√© sauvegard√©: summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Notebook termin√©!\n",
    "\n",
    "### üì• Pour t√©l√©charger votre mod√®le:\n",
    "1. Allez dans l'onglet **Output** (panneau de droite)\n",
    "2. T√©l√©chargez `alzheimer_model.h5`\n",
    "\n",
    "### üöÄ Prochaines √©tapes:\n",
    "- Essayer Transfer Learning (VGG16, ResNet)\n",
    "- Optimisation des hyperparam√®tres\n",
    "- D√©ploiement en application web\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
